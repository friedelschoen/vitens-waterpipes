# %%
import matplotlib.pyplot as plt
import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader
import random
import numpy as np
from dashboard.ai_model.DatasetLoaders import RealDataset, NeuralNetwork, SimulationDataset
# Load the CSV file into a pandas DataFrame
directory = 'dataset_output'
# file_name = 'good_dataset_20250421_174438.csv'
# file_path = f"{directory}/{file_name}"
# simulation_results = pd.read_csv(file_path)
# print(simulation_results.head())

device = torch.accelerator.current_accelerator(
).type if torch.accelerator.is_available() else "cpu"
print(f"Using {device} device")


# Split the dataset into training, validation, and test sets
# dataset = SimulationDataset(file_path)
dataset = RealDataset(normalize=True)
train_size = int(0.003 * len(dataset))
val_size = int(0.0005 * len(dataset))
test_size = len(dataset) - train_size - val_size

print(
    f"Train size: {train_size}, Validation size: {val_size}, Test size: {test_size}")

train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size, test_size]
)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# %%
model = NeuralNetwork().to(device)
mse_loss = nn.MSELoss()
loss_function = mse_loss

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
max_no_improvement = 5
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, factor=0.3, patience=max_no_improvement)
epochs = 500
loss_history = []

max_no_improvement = 6
no_improvement_count = 0
best_val_loss = float('inf')
early_stopping = False
# %%
for epoch in range(epochs):
    model.train()
    _loss_history = []
    _validation_loss_history = []
    # train
    for batch in train_loader:
        x, y = batch
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        logits = model(x)
        loss = loss_function(logits, y)
        loss.backward()
        optimizer.step()
        _loss_history.append(loss.item())
    # validation
    if epoch % 25 == 0:
        print(f"Epoch {epoch}, Loss: {np.mean(_loss_history)}")
        loss_history.append(np.mean(_loss_history))
        average_loss = 0.0
        for batch in val_loader:
            x, y = batch
            x, y = x.to(device), y.to(device)

            with torch.no_grad():
                logits = model(x)
                val_loss = loss_function(logits, y)
                average_loss += val_loss.item()
        average_loss /= len(val_loader)
        _validation_loss_history.append(average_loss)
        print(f"Validation Loss: {average_loss}")
        if average_loss < best_val_loss:
            best_val_loss = average_loss
            no_improvement_count = 0

        scheduler.step(val_loss)


# Plot the loss history over time
plt.figure(figsize=(10, 6))
plt.plot(loss_history, label="Training Loss")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.title("Training Loss History")
plt.legend()
plt.grid(True)
plt.show()

# %%
# Calculate accuracy on the training set
model.eval()
correct_predictions = 0
total_predictions = 0
MAE = []
MAPE = []
with torch.no_grad():
    for batch in test_loader:
        x, y = batch
        x, y = x.to(device), y.to(device)
        if torch.any(torch.isnan(x)) or torch.any(torch.isnan(y)):
            print("Encountered NaN values in the dataset.")
            # continue
        predictions = model(x)
        # Calculate Mean Absolute Error (MAE)
        MAE.append(torch.mean(torch.abs(predictions - y)).item())
        # Calculate Mean Absolute Percentage Error (MAPE)
        predictions = predictions.clamp(min=1e-8)  # Avoid division by zero
        y = y.clamp(min=1e-8)  # Avoid division by zero
        MAPE.append(torch.mean(torch.abs((predictions - y) / y)).item())
        # predicted_labels = predictions.round()  # Round predictions to nearest integer
        # correct_predictions += (predicted_labels == y).all(dim=1).sum().item()
        # total_predictions += y.size(0)

average_MAE = sum(MAE) / len(MAE)
average_MAPE = sum(MAPE) / len(MAPE)
print(f"Average MAE on training set: {average_MAE}")
print(f'Precision: {100.00 - (average_MAE * 100.00):.2f}%')
print(f"Average MAPE on training set: {average_MAPE}")
print(f'Precision: {100.00 - (average_MAPE * 100.00):.2f}%')

# %%
# Save the trained model
model_save_path = f"models/trained_model_{average_MAE}.pth"
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")

# %%
model.eval()
num_samples_to_show = 5

# Get random indices from the full dataset
# Ensure indices are unique and within the bounds of the dataset
if len(dataset) >= num_samples_to_show:
    random_indices = random.sample(range(len(dataset)), num_samples_to_show)
else:
    print(
        f"Warning: Requested {num_samples_to_show} samples, but dataset only has {len(dataset)} entries. Showing all.")
    random_indices = list(range(len(dataset)))

print(f"\n--- Comparing {num_samples_to_show} Random Samples ---")

with torch.no_grad():
    for i, idx in enumerate(random_indices):
        x_norm, y_norm = dataset[idx]
        x_norm_batch = x_norm.unsqueeze(0)

        prediction_norm = model(x_norm_batch)  # Shape is [1, 2]
        prediction_denorm = dataset.denormalize_output(prediction_norm)
        # Use unsqueeze for potential consistency
        actual_denorm = dataset.denormalize_output(y_norm.unsqueeze(0))
        original_input_features = dataset.original_dataset[idx, :8]

        print(f"\nSample #{i+1} (Dataset Index: {idx})")
        # Use numpy for potentially cleaner printing of arrays
        print(
            f"  Input Features (Original): {np.array2string(original_input_features, precision=4, suppress_small=True)}")
        # Move tensors to CPU and convert to numpy for printing
        print(
            f"  Predicted Output (Denormalized): {np.array2string(prediction_denorm.cpu().numpy(), precision=4, suppress_small=True)}")
        print(
            f"  Actual Output    (Denormalized): {np.array2string(actual_denorm.cpu().numpy(), precision=4, suppress_small=True)}")

print("\n--------------------------------------")
